ğŸ§© AI æ ¸å¿ƒæœåŠ¡è§„èŒƒï¼ˆai-core ç»Ÿä¸€æ¶æ„è§„èŒƒ v2.0ï¼‰

ç‰ˆæœ¬ï¼šv2.0
çŠ¶æ€ï¼šæ­£å¼å‘å¸ƒï¼ˆCurrent Standardï¼‰
é€‚ç”¨èŒƒå›´ï¼š

apps/ai-serviceï¼ˆRender éƒ¨ç½²çš„åœ¨çº¿æ¨¡å‹æœåŠ¡ï¼‰

apps/local-ai-serviceï¼ˆæœ¬åœ° Ollama è°ƒè¯•æœåŠ¡ï¼‰

æ‰€æœ‰è°ƒç”¨ /v1/ask çš„æœåŠ¡ï¼ˆdrivequiz-api / web / adminï¼‰

AI ç›¸å…³æœªæ¥æ–°æœåŠ¡ï¼ˆæ‰¹å¤„ç†å™¨ã€ç¿»è¯‘å™¨ã€RAG æœåŠ¡ç­‰ï¼‰

0. ä¿®è®¢èƒŒæ™¯

åœ¨ v1.0 ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å°† local-ai-service ä¸ ai-service çš„ AI è°ƒç”¨é€»è¾‘ç»Ÿä¸€åˆ°äº† sceneRunner.tsã€‚
ä½†åœ¨å·¥ç¨‹åŒ–å±‚é¢ä»å­˜åœ¨é—®é¢˜ï¼š

è·¨ app ç›¸å¯¹è·¯å¾„å¼•ç”¨ â†’ è„†å¼±ä¸”éš¾ä»¥ç»´æŠ¤

ç±»å‹é€€åŒ–ä¸º any â†’ æ˜“å¼•å‘è¿è¡Œæ—¶é—®é¢˜

OpenAI / Ollama provider å°è£…è€¦åˆåˆ° ai-service å†…éƒ¨

ä¸¤ä¸ªæœåŠ¡å¯¹ Config çš„ä¾èµ–è¾¹ç•Œä¸æ˜ç¡®

ä¸ºä¿è¯æœªæ¥å¤§è§„æ¨¡æ‰©å±•ï¼ˆæ‰¹å¤„ç†ã€æ¨¡å‹ä¼˜å…ˆçº§ã€è‡ªåŠ¨å›é€€ã€æœ¬åœ°ç¼“å­˜ç­‰ï¼‰ï¼Œéœ€è¦è¿›ä¸€æ­¥æå‡æ¶æ„ç¨³å®šæ€§ã€‚

å› æ­¤æ¨å‡º v2.0ï¼šai-core ç»Ÿä¸€æ¶æ„è§„èŒƒã€‚

1. æ¶æ„æ€»è§ˆï¼ˆSingle AI Pipeline Architectureï¼‰

æ‰€æœ‰ AI åœºæ™¯æ‰§è¡Œå¿…é¡»ä½¿ç”¨ ä¸€æ¡ç»Ÿä¸€çš„è°ƒç”¨ç®¡çº¿ï¼Œç”± shared package ç®¡ç†ï¼š

packages/ai-core
 â”œâ”€ src/
 â”‚   â”œâ”€ sceneRunner.ts      â† AI è°ƒç”¨å”¯ä¸€å…¥å£ï¼ˆä¸»é€»è¾‘ï¼‰
 â”‚   â”œâ”€ providers/
 â”‚   â”‚   â”œâ”€ openaiClient.ts â† OpenAI å°è£…
 â”‚   â”‚   â”œâ”€ ollamaClient.ts â† Ollama å°è£…
 â”‚   â”œâ”€ types.ts            â† æ ¸å¿ƒç±»å‹å®šä¹‰
 â”‚   â”œâ”€ utils.ts            â† å…¬å…±å·¥å…·æ–¹æ³•ï¼ˆå¯é€‰ï¼‰
 â”‚
 â”œâ”€ package.json
 â””â”€ index.ts


ä¸¤ä¸ªæœåŠ¡å¼•ç”¨ï¼š

apps/ai-service/src/routes/ask.ts
   â†’ import { runScene } from "@zalem/ai-core"

apps/local-ai-service/src/routes/ask.ts
   â†’ import { runScene } from "@zalem/ai-core"


æ‰€æœ‰è°ƒç”¨éƒ½åªèƒ½é€šè¿‡ runScene() è¿›å…¥ AI é€»è¾‘ã€‚
ç¦æ­¢ä»»ä½•æœåŠ¡è‡ªè¡Œå®ç° AI è°ƒç”¨ã€JSON æ„é€ ã€response_format ç­‰å†…å®¹ã€‚

2. ç›®å½•ç»“æ„ï¼ˆå¿…é¡»éµå¾ªï¼‰
packages/ai-core/
  â”œâ”€ src/
  â”‚   â”œâ”€ types.ts
  â”‚   â”œâ”€ sceneRunner.ts
  â”‚   â”œâ”€ utils.ts
  â”‚   â””â”€ providers/
  â”‚       â”œâ”€ openaiClient.ts
  â”‚       â””â”€ ollamaClient.ts
  â””â”€ index.ts

2.1 index.tsï¼ˆç»Ÿä¸€å¯¼å‡ºï¼‰
export * from "./src/types";
export * from "./src/sceneRunner";


apps å¼•ç”¨æ–¹å¼ï¼š

import { runScene, AiServiceConfig } from "@zalem/ai-core";

3. æ ¸å¿ƒæ¨¡å—è¯´æ˜
3.1 types.tsï¼ˆå¼ºç±»å‹å®šä¹‰ï¼‰
export interface AiServiceConfig {
  model: string;
  openaiApiKey?: string;     // ä»… openai provider å¿…éœ€
  ollamaUrl?: string;        // ä»… ollama provider å¿…éœ€

  userPrefix: string;
  refPrefix: string;
}

export interface SceneConfig {
  prompt: string;
  outputFormat: string | null;
}

export interface RunSceneOptions {
  sceneKey: string;
  locale: string;
  question: string;
  reference?: string | null;

  providerKind: "openai" | "ollama";
  config: AiServiceConfig;
}

export interface SceneResult {
  rawText: string;
  json: any | null;
}


â—ç¦æ­¢ä½¿ç”¨ anyï¼›å¿…é¡»å…¨éƒ¨ä½¿ç”¨ä¸Šè¿°ç±»å‹ã€‚

3.2 sceneRunner.tsï¼ˆAI è°ƒç”¨å”¯ä¸€å…¥å£ï¼‰

éœ€è¦åŒ…å«ï¼š

getSceneConfig()ï¼ˆä»æ•°æ®åº“è¯»å– prompt + outputFormatï¼‰

buildMessages()ï¼ˆç»Ÿä¸€æ„å»º system/user messagesï¼‰

getResponseFormatForScene()ï¼ˆåˆ¤æ–­æ˜¯å¦å¯ç”¨ JSON æ¨¡å¼ï¼‰

callModelWithProvider()ï¼ˆå°è£… OpenAI + Ollamaï¼‰

tryParseSceneResult()ï¼ˆç»Ÿä¸€ JSON.parseï¼‰

runScene()ï¼ˆå…¨é¡¹ç›®å”¯ä¸€ AI æ‰§è¡Œå…¥å£ï¼‰

æ‰€æœ‰æœªæ¥ä¿®æ”¹å¿…é¡»åœ¨æ­¤æ–‡ä»¶å®Œæˆã€‚

æ‰€æœ‰æœåŠ¡è°ƒç”¨æ–¹å¼ç»Ÿä¸€ï¼š

const result = await runScene({
  sceneKey: scene,
  locale: locale,
  question,
  reference,
  providerKind: "openai",       // Render
  config,
});


æˆ–ï¼š

providerKind: "ollama"          // Local

4. Provider å°è£…è§„èŒƒ
4.1 openaiClient.ts

èŒè´£ï¼š

è´Ÿè´£åˆ›å»º OpenAI SDK å®¢æˆ·ç«¯å®ä¾‹

è´Ÿè´£å¤„ç† response_format

è´Ÿè´£ç»Ÿä¸€çš„ tokens æå–

ç¦æ­¢å¯¼å…¥ ai-service å†…éƒ¨çš„æ¨¡å—ï¼ˆå¦‚ ServiceConfigï¼‰

æ¥å£ï¼š

export async function callOpenAI({
  model,
  messages,
  responseFormat,
  apiKey,
}: {
  model: string;
  messages: any[];
  responseFormat?: { type: "json_object" };
  apiKey: string;
}): Promise<string>;

4.2 ollamaClient.ts

èŒè´£ï¼š

å°è£… Ollama API è°ƒç”¨

ä¸å…è®¸ä¼ é€’ OpenAI é£æ ¼çš„å‚æ•°

è´Ÿè´£å…¼å®¹æ—§ç‰ˆä¸æ–°ç‰ˆ Ollama è¾“å‡ºç»“æ„

æ¥å£ï¼š

export async function callOllama({
  model,
  messages,
  url,
}: {
  model: string;
  messages: any[];
  url: string;
}): Promise<string>;

5. JSON è¾“å‡ºè§„èŒƒ
5.1 ç»Ÿä¸€è§„åˆ™ï¼ˆå¿…é¡»å†™æ­»åœ¨ sceneRunner å†…ï¼‰
if (outputFormat.includes("json")) {
    response_format = { type: "json_object" };
}

5.2 è¿”å›æ ¼å¼å¿…é¡»ç»Ÿä¸€
{
  "ok": true,
  "data": {
    "answer": "<rawText>",
    "json": { ... } | null
  }
}


ç¦æ­¢ï¼š

è¿”å› OpenAI åŸå§‹ç»“æ„

è¿”å› choices

è¿”å› usage

è¿”å› model ä¿¡æ¯

ï¼ˆè¿™äº›ä¿¡æ¯ç»Ÿä¸€åœ¨æœåŠ¡æ—¥å¿—é‡Œè®°å½•ã€‚ï¼‰

6. /v1/ask è·¯ç”±è§„èŒƒï¼ˆå¼ºåˆ¶ç»Ÿä¸€ï¼‰

æ¯ä¸ªæœåŠ¡ï¼ˆlocal / renderï¼‰ä¸­çš„ /v1/ask è·¯ç”±å¿…é¡»éµå¾ªï¼š

6.1 å…è®¸åšçš„ï¼š

âœ” å‚æ•°æ ¡éªŒ
âœ” ä»æ•°æ®åº“åŠ è½½ scene keyã€localeã€questionã€reference
âœ” è°ƒç”¨ runScene()
âœ” è¿”å› HTTP å“åº”ç»“æ„

6.2 ç¦æ­¢ï¼ˆè¿åè§„èŒƒå³ç®—åˆ†å‰ï¼‰ï¼š

ğŸš« ä¸å…è®¸ç›´æ¥è°ƒç”¨ OpenAI SDK
ğŸš« ä¸å…è®¸ç›´æ¥è°ƒç”¨ Ollama API
ğŸš« ä¸å…è®¸è‡ªè¡Œæ„é€  response_format
ğŸš« ä¸å…è®¸è‡ªè¡Œ parse JSON
ğŸš« ä¸å…è®¸è°ƒæ•´ prompt
ğŸš« ä¸å…è®¸æ„é€  messages
ğŸš« ä¸å…è®¸ä»è·¯ç”±å±‚è¯»å– outputFormat
ğŸš« ä¸å…è®¸åœ¨è·¯ç”±å±‚åš provider fallback
ğŸš« ä¸å…è®¸åœ¨è·¯ç”±å±‚åš model é€‰æ‹©

æ‰€æœ‰è¿™äº›é€»è¾‘å¿…é¡»åœ¨ packages/ai-core/sceneRunner.ts å†…ã€‚

7. Config è¾¹ç•Œè§„èŒƒ

æ¯ä¸ªæœåŠ¡å¿…é¡»å®ç° AiServiceConfigï¼š

const config: AiServiceConfig = {
  model: process.env.MODEL,
  openaiApiKey: process.env.OPENAI_KEY,
  ollamaUrl: process.env.OLLAMA_URL,

  userPrefix: "User:",
  refPrefix: "Reference:",
};

å¿…é¡»ä¿è¯ï¼š

ai-serviceï¼ˆrenderï¼‰å¿…é¡»è®¾ç½® openaiApiKey

local-ai-service å¿…é¡»è®¾ç½® ollamaUrl

ä¸¤è€…éƒ½å¿…é¡»è®¾ç½® model/userPrefix/refPrefix

ä»»ä½•å…¶ä»–å†…éƒ¨å­—æ®µä¸å¾—åŠ å…¥ AiServiceConfig ä¸­

AiServiceConfig å¿…é¡»ä¿æŒé€æ˜ã€è½»é‡ã€ç¨³å®šã€‚

8. é”™è¯¯å¤„ç†è§„èŒƒ

å¯¹äºä»»ä½•é”™è¯¯ï¼ˆOpenAI/Ollama å¤±è´¥ã€DB è¯»å–å¤±è´¥ã€JSON è§£æå¤±è´¥ç­‰ï¼‰ï¼š

è¿”å›ç»Ÿä¸€ç»“æ„ï¼š

{
  "ok": false,
  "error": {
    "message": "...",
    "code": "AI_SERVICE_ERROR",
    "provider": "openai | ollama"
  }
}


é”™è¯¯ç»“æ„ä¸å¾—ä¸ local / remote äº§ç”Ÿå·®å¼‚ã€‚

9. è‡ªæ£€ï¼ˆCI è§„èŒƒï¼‰

æ¯æ¬¡æäº¤å¿…é¡»è¿è¡Œä»¥ä¸‹æ£€æŸ¥ã€‚

9.1 ç¦æ­¢é‡å¤ AI å®ç°ï¼ˆå¿…é¡» 0 ç»“æœï¼‰
rg "chat\.completions\.create" apps
rg "ollama" apps              # é™¤ providers/ollamaClient.ts å¤–å¿…é¡» 0 ç»“æœ
rg "response_format" apps/    # é™¤ sceneRunner.ts å¤–å¿…é¡» 0 ç»“æœ

9.2 ç¦æ­¢è·¨ app ç›¸å¯¹è·¯å¾„
rg "\.\./\.\./\.\./ai-service" -n


ç»“æœå¿…é¡»ä¸º 0ã€‚

9.3 è·¯ç”±å†…ç¦æ­¢å‡ºç° AI é€»è¾‘
rg "new OpenAI" apps
rg "JSON.parse" apps          # é™¤ sceneRunner.ts å¿…é¡»ä¸º 0

10. å›å½’æµ‹è¯•è§„èŒƒï¼ˆè‡ªåŠ¨åŒ– or æ‰‹åŠ¨ï¼‰

åŒä¸€è¾“å…¥ï¼š

æ‰“å‘ local-ai-service /v1/ask

æ‰“å‘ ai-serviceï¼ˆRenderï¼‰ /v1/ask

ä¸¤è€…å¿…é¡»ï¼š

ok: true

data.answer éç©º

å¯¹äº JSON åœºæ™¯ï¼š

data.json å¿…é¡»æ˜¯ objectï¼ˆé nullï¼‰

è‹¥ä»»ä½•ä¸ä¸€è‡´ â†’ è§†ä¸ºè¿åè§„èŒƒã€‚

11. æœªæ¥æ‰©å±•ç‚¹ï¼ˆå¿…é¡»ä»¥ ai-core ä¸ºä¸­å¿ƒï¼‰

æœªæ¥æ‰€æœ‰æ–°åŠŸèƒ½å¿…é¡»åœ¨ ai-core æ‰©å±•ï¼š

åŠŸèƒ½	å¿…é¡»æ”¾åœ¨ ai-core
å¤š Provider ä¼˜å…ˆçº§	âœ”
Provider è‡ªåŠ¨å›é€€	âœ”
æ‰¹é‡å¤„ç†	âœ”
Prompt æ¨¡ç‰ˆç³»ç»Ÿ	âœ”
ç»Ÿä¸€ç¼“å­˜	âœ”
ç»Ÿä¸€ tokens è®¡æ•°ç­–ç•¥	âœ”
RAG ç»Ÿä¸€æ•´åˆ	âœ”

ä»»ä½•æ”¾åœ¨ ai-service æˆ– local-ai-service å†…éƒ¨çš„ Provider é€»è¾‘éƒ½è¿åè§„èŒƒã€‚

âœ” æœ€ç»ˆå£°æ˜ï¼ˆå¿…é¡»å†™å…¥è§„èŒƒï¼‰

è‡ªæœ¬è§„èŒƒå‘å¸ƒåï¼Œæ‰€æœ‰ AI ç›¸å…³é€»è¾‘åªèƒ½ä¿®æ”¹ packages/ai-coreã€‚
ai-service ä¸ local-ai-service ä¸å¾—è‡ªè¡Œç¼–å†™ AI é€»è¾‘ï¼Œä¸å¾—åˆ†å‰ï¼Œä¸å¾—ç»•è¿‡ runSceneã€‚
ä»»ä½•è¿åæ­¤è§„èŒƒçš„ä»£ç å‡è§†ä¸ºæ¶æ„è¿è§„ã€‚