# ğŸš€ æœ¬åœ°AIæœåŠ¡ï¼ˆOllamaï¼‰ç ”å‘æŒ‡ä»¤

**åˆ›å»ºæ—¶é—´**: 2025-01-15  
**æ‰§è¡Œçª—å£**: ä¸‹ä¸€ä¸ªå¼€å‘çª—å£  
**çŠ¶æ€**: å¾…æ‰§è¡Œ  
**é‡è¦**: âš ï¸ **ä»…è¾“å‡ºæŒ‡ä»¤ï¼Œä¸è¦åœ¨å½“å‰çª—å£æ‰§è¡Œ**

---

## ğŸ“‹ æ‰§è¡Œå‰å¿…è¯»

### æ ¸å¿ƒåŸåˆ™

1. **ç‹¬ç«‹æ¶æ„**ï¼šæœ¬åœ°AIæœåŠ¡å¿…é¡»å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¸ç°æœ‰çš„ `apps/ai-service`ï¼ˆåœ¨çº¿AIæœåŠ¡ï¼‰è€¦åˆ
2. **ä¸ä¿®æ”¹ç°æœ‰ä»£ç **ï¼šä¸è¦ä¿®æ”¹ `apps/ai-service` çš„ä»»ä½•ä»£ç 
3. **ç‹¬ç«‹éƒ¨ç½²**ï¼šæœ¬åœ°AIæœåŠ¡å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ– Render ç­‰äº‘ç«¯æœåŠ¡
4. **å®Œå…¨æœ¬åœ°åŒ–**ï¼šä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€ OpenAI API

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ¶æ„å¯¹æ¯”

#### å½“å‰æ¶æ„ï¼ˆåœ¨çº¿AIæœåŠ¡ï¼‰

```
ç”¨æˆ·æµè§ˆå™¨
    â†“
Vercel (ä¸»ç«™) - apps/web
    â†“ /api/ai/ask
Render (AI-Service) - apps/ai-service
    â†“
OpenAI API (äº‘ç«¯)
```

#### æ–°æ¶æ„ï¼ˆæœ¬åœ°AIæœåŠ¡ - ç‹¬ç«‹ï¼‰

```
ç”¨æˆ·æµè§ˆå™¨
    â†“
æœ¬åœ° Next.js (ä¸»ç«™) - apps/web
    â†“ /api/ai/ask  (ä¿®æ”¹ç°æœ‰è·¯ç”±ï¼Œæ”¯æŒæ— ç¼åˆ‡æ¢)
æœ¬åœ°AIæœåŠ¡ - apps/local-ai-service (æ–°æœåŠ¡)
    â†“
æœ¬åœ° Ollama (localhost:11434)
```

### æ¶æ„ç‰¹ç‚¹

1. **å®Œå…¨ç‹¬ç«‹**ï¼š
   - æ–°æœåŠ¡ï¼š`apps/local-ai-service`
   - æ–°ç«¯å£ï¼š`8788`ï¼ˆä¸ `apps/ai-service` çš„ 8787 åŒºåˆ†ï¼‰
   - ä¿®æ”¹ç°æœ‰è·¯ç”±ï¼š`/api/ai/ask`ï¼ˆæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡æ— ç¼åˆ‡æ¢ï¼‰
   - ç‹¬ç«‹é…ç½®ï¼š`.env.local` ç‹¬ç«‹ç¯å¢ƒå˜é‡

2. **ä¸è€¦åˆç°æœ‰æœåŠ¡**ï¼š
   - âœ… ä¸å¼•ç”¨ `apps/ai-service` çš„ä»£ç 
   - âœ… ä¸å…±äº«é…ç½®æ–‡ä»¶
   - âœ… ä¸å…±ç”¨æ•°æ®åº“è¡¨ï¼ˆå¯é€‰ï¼Œæˆ–ä½¿ç”¨ç‹¬ç«‹è¡¨ï¼‰
   - âœ… ç‹¬ç«‹çš„è·¯ç”±å’Œä¸­é—´ä»¶

3. **æŠ€æœ¯æ ˆ**ï¼š
   - æ¡†æ¶ï¼šFastifyï¼ˆä¸ç°æœ‰ä¿æŒä¸€è‡´ï¼Œä½†ç‹¬ç«‹å®ç°ï¼‰
   - Chatæ¨¡å‹ï¼š`llama3.2:3b`ï¼ˆOllamaï¼‰
   - Embeddingæ¨¡å‹ï¼š`nomic-embed-text`ï¼ˆOllamaï¼Œ768ç»´ï¼‰
   - å‘é‡æ•°æ®åº“ï¼šSupabase pgvectorï¼ˆ768ç»´ï¼‰

---

## ğŸ“¦ æ–¹æ¡ˆç»†èŠ‚

### æ–¹æ¡ˆï¼šè½»é‡çº§æœ¬åœ°AIæœåŠ¡

#### 1. æ¨¡å‹é…ç½®

```bash
# Chat æ¨¡å‹ï¼ˆç”Ÿæˆå›ç­”ï¼‰
æ¨¡å‹åç§°: llama3.2:3b
å†…å­˜å ç”¨: ~2-3GB
ç”¨é€”: æ ¹æ®é—®é¢˜å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”

# Embedding æ¨¡å‹ï¼ˆRAGæ£€ç´¢ï¼‰
æ¨¡å‹åç§°: nomic-embed-text
å‘é‡ç»´åº¦: 768
å†…å­˜å ç”¨: ~500MB
ç”¨é€”: å°†é—®é¢˜å’Œæ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œç”¨äºç›¸ä¼¼åº¦æœç´¢
```

#### 2. ç›®å½•ç»“æ„

```
/Users/leo/Desktop/kkdrivequiz/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ ai-service/          # ç°æœ‰åœ¨çº¿AIæœåŠ¡ï¼ˆä¸ä¿®æ”¹ï¼‰
â”‚   â””â”€â”€ local-ai-service/    # æ–°æœåŠ¡ï¼ˆç‹¬ç«‹åˆ›å»ºï¼‰
â”‚       â”œâ”€â”€ package.json
â”‚       â”œâ”€â”€ tsconfig.json
â”‚       â”œâ”€â”€ .env.local       # ç‹¬ç«‹ç¯å¢ƒå˜é‡
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ index.ts
â”‚           â”œâ”€â”€ lib/
â”‚           â”‚   â”œâ”€â”€ ollamaClient.ts    # Ollamaå®¢æˆ·ç«¯
â”‚           â”‚   â”œâ”€â”€ rag.ts             # RAGæ£€ç´¢ï¼ˆ768ç»´ï¼‰
â”‚           â”‚   â”œâ”€â”€ logger.ts
â”‚           â”‚   â””â”€â”€ config.ts
â”‚           â”œâ”€â”€ routes/
â”‚           â”‚   â””â”€â”€ ask.ts             # /v1/ask è·¯ç”±
â”‚           â””â”€â”€ middlewares/
â”‚               â””â”€â”€ auth.ts
â”œâ”€â”€ src/
â”‚   â””â”€â”€ migrations/
â”‚       â””â”€â”€ 20250115_migrate_to_ollama_768d.sql  # æ•°æ®åº“è¿ç§»ï¼ˆå¦‚æœè¿˜æ²¡æ‰§è¡Œï¼‰
â””â”€â”€ apps/web/
    â””â”€â”€ app/
        â””â”€â”€ api/
            â””â”€â”€ ai/
                â””â”€â”€ ask/
                    â””â”€â”€ route.ts       # ä¿®æ”¹ç°æœ‰è·¯ç”±æ”¯æŒæœåŠ¡åˆ‡æ¢
```

#### 3. ç¯å¢ƒå˜é‡é…ç½®

**`apps/local-ai-service/.env.local`**:
```bash
# Ollama é…ç½®
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_API_KEY=ollama  # ä»»æ„å€¼ï¼ŒOllamaä¸éœ€è¦çœŸå®key

# æ¨¡å‹é…ç½®
AI_MODEL=llama3.2:3b
EMBEDDING_MODEL=nomic-embed-text

# æœåŠ¡é…ç½®
PORT=8788
HOST=0.0.0.0
NODE_ENV=development
SERVICE_TOKENS=local_ai_token_xxx  # ç‹¬ç«‹tokenï¼Œä¸ä¸ai-serviceå…±ç”¨

# æ•°æ®åº“é…ç½®
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your_service_key
```

**é¡¹ç›®æ ¹ç›®å½• `.env.local`ï¼ˆä¸»ç«™é…ç½®ï¼‰**:
```bash
# æœ¬åœ°AIæœåŠ¡åœ°å€ï¼ˆæ–°å¢ï¼‰
LOCAL_AI_SERVICE_URL=http://localhost:8788
LOCAL_AI_SERVICE_TOKEN=local_ai_token_xxx

# ç°æœ‰é…ç½®ä¿æŒä¸å˜
AI_SERVICE_URL=https://ai.zalem.app  # åœ¨çº¿AIæœåŠ¡ï¼ˆä¿ç•™ï¼‰
AI_SERVICE_TOKEN=svc_token_xxx       # åœ¨çº¿AI Tokenï¼ˆä¿ç•™ï¼‰
```

#### 4. æ•°æ®åº“è¿ç§»

**å‰æ**ï¼šæ•°æ®åº“ä¸ºç©ºæˆ–å¯ä»¥å®‰å…¨è¿ç§»

**è¿ç§»è„šæœ¬**ï¼š`src/migrations/20250115_migrate_to_ollama_768d.sql`

**æ‰§è¡Œä½ç½®**ï¼šSupabase SQL Editor æˆ–é€šè¿‡ psql

**è¿ç§»å†…å®¹**ï¼š
- å°† `ai_vectors` è¡¨çš„ `embedding` åˆ—ä» `vector(1536)` æ”¹ä¸º `vector(768)`
- æ›´æ–° `match_documents` å‡½æ•°å‚æ•°ä» `vector(1536)` æ”¹ä¸º `vector(768)`
- é‡å»ºå‘é‡ç´¢å¼•ï¼ˆivfflatï¼‰

---

## ğŸ“ æ‰§è¡Œé¡ºåº

### é˜¶æ®µ1ï¼šç¯å¢ƒå‡†å¤‡ï¼ˆ15åˆ†é’Ÿï¼‰

#### æ­¥éª¤1.1ï¼šå®‰è£…å’Œå¯åŠ¨ Ollama

```bash
# 1. æ£€æŸ¥ Ollama æ˜¯å¦å·²å®‰è£…
ollama --version

# 2. å¦‚æœæœªå®‰è£…ï¼Œæ‰§è¡Œå®‰è£…
# macOS:
brew install ollama

# æˆ–ä½¿ç”¨å®˜æ–¹å®‰è£…è„šæœ¬:
curl -fsSL https://ollama.com/install.sh | sh

# 3. å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# 4. éªŒè¯ Ollama è¿è¡Œ
curl http://localhost:11434/api/tags
```

#### æ­¥éª¤1.2ï¼šæ‹‰å–æ¨¡å‹

```bash
# æ‹‰å– Chat æ¨¡å‹
ollama pull llama3.2:3b

# æ‹‰å– Embedding æ¨¡å‹
ollama pull nomic-embed-text

# éªŒè¯æ¨¡å‹
ollama list
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… Ollama æœåŠ¡è¿è¡Œåœ¨ `localhost:11434`
- âœ… ä¸¤ä¸ªæ¨¡å‹éƒ½å·²ä¸‹è½½ï¼ˆ`llama3.2:3b` å’Œ `nomic-embed-text`ï¼‰

---

### é˜¶æ®µ2ï¼šæ•°æ®åº“è¿ç§»ï¼ˆ10åˆ†é’Ÿï¼‰

#### æ­¥éª¤2.1ï¼šæ‰§è¡Œè¿ç§»è„šæœ¬

**ä½ç½®**ï¼šSupabase Dashboard â†’ SQL Editor

**è„šæœ¬è·¯å¾„**ï¼š`src/migrations/20250115_migrate_to_ollama_768d.sql`

**æ‰§è¡Œæ–¹å¼**ï¼š
1. æ‰“å¼€ Supabase Dashboard
2. è¿›å…¥ SQL Editor
3. å¤åˆ¶è¿ç§»è„šæœ¬å†…å®¹
4. æ‰§è¡Œè„šæœ¬

**æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ**ï¼š
```bash
psql $DATABASE_URL -f src/migrations/20250115_migrate_to_ollama_768d.sql
```

#### æ­¥éª¤2.2ï¼šéªŒè¯è¿ç§»

```sql
-- æ£€æŸ¥è¡¨ç»“æ„
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_name = 'ai_vectors' AND column_name = 'embedding';

-- é¢„æœŸç»“æœ: vector(768)

-- æ£€æŸ¥å‡½æ•°å‚æ•°
SELECT pg_get_function_arguments(oid) 
FROM pg_proc 
WHERE proname = 'match_documents';

-- é¢„æœŸç»“æœ: query_embedding vector(768), ...
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… `ai_vectors.embedding` åˆ—ç±»å‹ä¸º `vector(768)`
- âœ… `match_documents` å‡½æ•°å‚æ•°ä¸º `vector(768)`
- âœ… å‘é‡ç´¢å¼•å·²é‡å»º

---

### é˜¶æ®µ3ï¼šåˆ›å»ºæœ¬åœ°AIæœåŠ¡ï¼ˆ30åˆ†é’Ÿï¼‰

#### æ­¥éª¤3.1ï¼šåˆ›å»ºç›®å½•ç»“æ„

```bash
cd /Users/leo/Desktop/kkdrivequiz/apps
mkdir -p local-ai-service/src/{lib,routes,middlewares}
```

#### æ­¥éª¤3.2ï¼šåˆ›å»º package.json

**æ–‡ä»¶**ï¼š`apps/local-ai-service/package.json`

```json
{
  "name": "local-ai-service",
  "version": "1.0.0",
  "description": "æœ¬åœ°AIé—®ç­”æœåŠ¡ - åŸºäºOllama",
  "private": true,
  "type": "module",
  "main": "src/index.ts",
  "scripts": {
    "dev": "tsx watch src/index.ts",
    "build": "tsc",
    "start": "node dist/index.js",
    "type-check": "tsc --noEmit"
  },
  "dependencies": {
    "@fastify/cors": "^10.0.1",
    "dotenv": "^17.2.3",
    "fastify": "^5.1.0",
    "pino": "^9.6.0",
    "pino-pretty": "^13.0.0"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "tsx": "^4.7.0",
    "typescript": "^5.5.3"
  }
}
```

#### æ­¥éª¤3.3ï¼šåˆ›å»º tsconfig.json

**æ–‡ä»¶**ï¼š`apps/local-ai-service/tsconfig.json`

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "lib": ["ES2022"],
    "moduleResolution": "node",
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

#### æ­¥éª¤3.4ï¼šåˆ›å»º Ollama å®¢æˆ·ç«¯

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/lib/ollamaClient.ts`

```typescript
/**
 * Ollama å®¢æˆ·ç«¯å°è£…
 * æä¾›ç»Ÿä¸€çš„ Ollama API è°ƒç”¨æ¥å£
 */

const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || "http://localhost:11434/v1";
const AI_MODEL = process.env.AI_MODEL || "llama3.2:3b";
const EMBEDDING_MODEL = process.env.EMBEDDING_MODEL || "nomic-embed-text";

/**
 * è°ƒç”¨ Ollama Chat API
 */
export async function callOllamaChat(
  messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
  temperature = 0.4
): Promise<string> {
  const response = await fetch(`${OLLAMA_BASE_URL}/chat/completions`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: AI_MODEL,
      messages,
      temperature,
    }),
  });

  if (!response.ok) {
    const error = await response.text().catch(() => "Unknown error");
    throw new Error(`Ollama Chat API è°ƒç”¨å¤±è´¥: ${error}`);
  }

  const data = await response.json();
  return data.choices?.[0]?.message?.content?.trim() || "";
}

/**
 * è°ƒç”¨ Ollama Embedding API
 */
export async function callOllamaEmbedding(text: string): Promise<number[]> {
  const response = await fetch(`${OLLAMA_BASE_URL}/embeddings`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: EMBEDDING_MODEL,
      prompt: text.slice(0, 3000), // é™åˆ¶é•¿åº¦
    }),
  });

  if (!response.ok) {
    const error = await response.text().catch(() => "Unknown error");
    throw new Error(`Ollama Embedding API è°ƒç”¨å¤±è´¥: ${error}`);
  }

  const data = await response.json();
  const embedding = data.embedding;

  if (!Array.isArray(embedding) || embedding.length !== 768) {
    throw new Error(`Embedding ç»´åº¦é”™è¯¯: æœŸæœ› 768 ç»´ï¼Œå®é™… ${embedding.length} ç»´`);
  }

  return embedding as number[];
}
```

#### æ­¥éª¤3.5ï¼šåˆ›å»º RAG æ£€ç´¢æ¨¡å—

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/lib/rag.ts`

```typescript
/**
 * RAG æ£€ç´¢æ¨¡å—ï¼ˆSupabase pgvectorï¼Œ768ç»´ï¼‰
 * ä½¿ç”¨ Ollama nomic-embed-text ç”Ÿæˆå‘é‡
 */

import { callOllamaEmbedding } from "./ollamaClient.js";

const SUPABASE_URL = process.env.SUPABASE_URL || "";
const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_KEY || "";
const DEFAULT_MATCH_COUNT = 5;
const CONTEXT_CHAR_LIMIT = 4000;

type RagHit = {
  content: string;
  source_title?: string | null;
  source_url?: string | null;
  similarity?: number | null;
};

/**
 * è°ƒç”¨ Supabase RPCï¼šmatch_documentsï¼ˆ768ç»´ï¼‰
 */
async function callSupabaseMatch(
  queryEmbedding: number[],
  lang: string = "zh",
  matchCount: number = DEFAULT_MATCH_COUNT
): Promise<RagHit[]> {
  if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) return [];

  const url = `${SUPABASE_URL.replace(/\/+$/, "")}/rest/v1/rpc/match_documents`;
  const body = {
    query_embedding: queryEmbedding,
    match_threshold: 0.75,
    match_count: Math.max(1, Math.min(10, matchCount)),
  };

  const headers: Record<string, string> = {
    "content-type": "application/json",
    apikey: SUPABASE_SERVICE_KEY,
    authorization: `Bearer ${SUPABASE_SERVICE_KEY}`,
    accept: "application/json",
  };

  try {
    const res = await fetch(url, {
      method: "POST",
      headers,
      body: JSON.stringify(body),
      signal: AbortSignal.timeout(10000),
    });

    if (res.status === 404) return [];
    if (!res.ok) {
      const text = await res.text().catch(() => "");
      throw new Error(`Supabase RPC error ${res.status}: ${text}`);
    }

    const data = (await res.json()) as unknown;
    if (!Array.isArray(data)) return [];

    return (data as RagHit[]).map((d) => ({
      content: String(d.content || ""),
      source_title: d.source_title ?? null,
      source_url: d.source_url ?? null,
      similarity: typeof d.similarity === "number" ? d.similarity : null,
    }));
  } catch (error) {
    console.error("RAGæ£€ç´¢å¤±è´¥:", error);
    return [];
  }
}

/**
 * æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
 */
function buildContext(hits: RagHit[]): string {
  if (!hits.length) return "";

  const parts: string[] = [];
  for (const h of hits) {
    const src = h.source_title ? `ã€æ¥æº:${h.source_title}ã€‘` : "";
    const sc = typeof h.similarity === "number" ? `ï¼ˆç›¸å…³åº¦:${h.similarity.toFixed(3)}ï¼‰` : "";
    const chunk = `${src}${sc}\n${String(h.content || "").trim()}`;
    parts.push(chunk);

    const tmp = parts.join("\n\n---\n\n");
    if (tmp.length >= CONTEXT_CHAR_LIMIT) {
      return tmp.slice(0, CONTEXT_CHAR_LIMIT);
    }
  }

  const joined = parts.join("\n\n---\n\n");
  return joined.length > CONTEXT_CHAR_LIMIT ? joined.slice(0, CONTEXT_CHAR_LIMIT) : joined;
}

/**
 * è·å– RAG ä¸Šä¸‹æ–‡
 */
export async function getRagContext(
  question: string,
  lang: string = "zh"
): Promise<string> {
  try {
    // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆ768ç»´ï¼‰
    const embedding = await callOllamaEmbedding(question);

    // 2. è°ƒç”¨ Supabase RPC æ£€ç´¢
    const hits = await callSupabaseMatch(embedding, lang, DEFAULT_MATCH_COUNT);

    // 3. æ„å»ºä¸Šä¸‹æ–‡
    return buildContext(hits);
  } catch (error) {
    console.error("RAGä¸Šä¸‹æ–‡è·å–å¤±è´¥:", error);
    return "";
  }
}
```

#### æ­¥éª¤3.6ï¼šåˆ›å»ºé…ç½®å’Œæ—¥å¿—æ¨¡å—

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/lib/config.ts`

```typescript
import dotenv from "dotenv";

dotenv.config();

export type LocalAIConfig = {
  port: number;
  host: string;
  serviceTokens: Set<string>;
  ollamaBaseUrl: string;
  aiModel: string;
  embeddingModel: string;
  supabaseUrl: string;
  supabaseServiceKey: string;
  nodeEnv: string;
  version: string;
};

export function loadConfig(): LocalAIConfig {
  const {
    PORT,
    HOST,
    SERVICE_TOKENS,
    OLLAMA_BASE_URL,
    AI_MODEL,
    EMBEDDING_MODEL,
    SUPABASE_URL,
    SUPABASE_SERVICE_KEY,
    NODE_ENV,
    npm_package_version,
  } = process.env;

  if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) {
    throw new Error("SUPABASE_URL å’Œ SUPABASE_SERVICE_KEY å¿…é¡»é…ç½®");
  }

  return {
    port: Number(PORT || 8788),
    host: HOST || "0.0.0.0",
    serviceTokens: new Set(
      (SERVICE_TOKENS || "")
        .split(",")
        .map((s) => s.trim())
        .filter(Boolean)
    ),
    ollamaBaseUrl: OLLAMA_BASE_URL || "http://localhost:11434/v1",
    aiModel: AI_MODEL || "llama3.2:3b",
    embeddingModel: EMBEDDING_MODEL || "nomic-embed-text",
    supabaseUrl: SUPABASE_URL,
    supabaseServiceKey: SUPABASE_SERVICE_KEY,
    nodeEnv: NODE_ENV || "development",
    version: npm_package_version || "0.0.0",
  };
}
```

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/lib/logger.ts`

```typescript
import pino from "pino";

export const logger = pino({
  level: process.env.LOG_LEVEL || "info",
  transport: process.env.NODE_ENV === "development" ? {
    target: "pino-pretty",
    options: {
      colorize: true,
    },
  } : undefined,
});
```

#### æ­¥éª¤3.7ï¼šåˆ›å»ºè®¤è¯ä¸­é—´ä»¶

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/middlewares/auth.ts`

```typescript
import { FastifyRequest } from "fastify";
import type { LocalAIConfig } from "../lib/config.js";

export function ensureServiceAuth(
  request: FastifyRequest,
  config: LocalAIConfig
): void {
  const authHeader = request.headers.authorization;
  if (!authHeader || !authHeader.startsWith("Bearer ")) {
    const err: Error & { statusCode?: number } = new Error("Missing Authorization header");
    err.statusCode = 401;
    throw err;
  }

  const token = authHeader.slice(7);
  if (!config.serviceTokens.has(token)) {
    const err: Error & { statusCode?: number } = new Error("Invalid service token");
    err.statusCode = 401;
    throw err;
  }
}
```

#### æ­¥éª¤3.8ï¼šåˆ›å»ºé—®ç­”è·¯ç”±

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/routes/ask.ts`

```typescript
import type { FastifyInstance, FastifyRequest, FastifyReply } from "fastify";
import { ensureServiceAuth } from "../middlewares/auth.js";
import { getRagContext } from "../lib/rag.js";
import { callOllamaChat } from "../lib/ollamaClient.js";
import type { LocalAIConfig } from "../lib/config.js";

type AskBody = {
  question?: string;
  userId?: string;
  lang?: string;
};

type AskResult = {
  answer: string;
  sources?: Array<{ title: string; url: string; snippet?: string }>;
  model: string;
  safetyFlag: "ok" | "needs_human" | "blocked";
};

function buildSystemPrompt(lang: string): string {
  const base =
    "ä½ æ˜¯ ZALEM é©¾é©¶è€ƒè¯•å­¦ä¹ åŠ©æ‰‹ã€‚è¯·åŸºäºæ—¥æœ¬äº¤é€šæ³•è§„ä¸é¢˜åº“çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¼•ç”¨æ—¶è¦ç®€æ´ï¼Œä¸ç¼–é€ ï¼Œä¸è¾“å‡ºä¸é©¾é©¶è€ƒè¯•æ— å…³çš„å†…å®¹ã€‚";
  if (lang === "ja") {
    return "ã‚ãªãŸã¯ ZALEM ã®é‹è»¢å…è¨±å­¦ç¿’ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬ã®äº¤é€šæ³•è¦ã¨å•é¡Œé›†ã®çŸ¥è­˜ã«åŸºã¥ã„ã¦ã€ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã‚„æé€ ã¯ç¦æ­¢ã—ã€é–¢ä¿‚ã®ãªã„å†…å®¹ã¯å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚";
  }
  if (lang === "en") {
    return "You are ZALEM's driving-test study assistant. Answer based on Japan's traffic laws and question bank. Be concise and accurate. Do not fabricate or include unrelated content.";
  }
  return base;
}

export default async function askRoute(app: FastifyInstance): Promise<void> {
  app.post(
    "/v1/ask",
    async (request: FastifyRequest<{ Body: AskBody }>, reply: FastifyReply): Promise<void> => {
      const config = app.config as LocalAIConfig;

      try {
        // 1) æœåŠ¡é—´é‰´æƒ
        ensureServiceAuth(request, config);

        // 2) æ ¡éªŒè¯·æ±‚ä½“
        const body = request.body as AskBody;
        const question = (body.question || "").trim();
        const lang = (body.lang || "zh").toLowerCase().trim();

        if (!question || question.length === 0 || question.length > 2000) {
          reply.code(400).send({
            ok: false,
            errorCode: "VALIDATION_FAILED",
            message: "Question is required and must be between 1 and 2000 characters",
          });
          return;
        }

        // 3) RAG æ£€ç´¢ï¼ˆè·å–ä¸Šä¸‹æ–‡ï¼‰
        const reference = await getRagContext(question, lang);

        // 4) è°ƒç”¨ Ollama Chat
        const sys = buildSystemPrompt(lang);
        const userPrefix = lang === "ja" ? "è³ªå•ï¼š" : lang === "en" ? "Question:" : "é—®é¢˜ï¼š";
        const refPrefix =
          lang === "ja" ? "é–¢é€£å‚ç…§ï¼š" : lang === "en" ? "Related references:" : "ç›¸å…³å‚è€ƒèµ„æ–™ï¼š";

        const answer = await callOllamaChat(
          [
            { role: "system", content: sys },
            {
              role: "user",
              content: `${userPrefix} ${question}\n\n${refPrefix}\n${reference || "ï¼ˆç„¡/Noneï¼‰"}`,
            },
          ],
          0.4
        );

        if (!answer) {
          reply.code(502).send({
            ok: false,
            errorCode: "PROVIDER_ERROR",
            message: "Empty response from Ollama",
          });
          return;
        }

        // 5) æ„å»º sourcesï¼ˆä» reference ä¸­æå–ï¼‰
        const sources: Array<{ title: string; url: string; snippet?: string }> = reference
          ? [{ title: "RAG Reference", url: "", snippet: reference.slice(0, 200) }]
          : [];

        // 6) è¿”å›ç»“æœ
        const result: AskResult = {
          answer,
          sources: sources.length > 0 ? sources : undefined,
          model: config.aiModel,
          safetyFlag: "ok", // æœ¬åœ°æœåŠ¡æš‚ä¸å®ç°å®‰å…¨å®¡æŸ¥
        };

        reply.send({
          ok: true,
          data: result,
        });
      } catch (e) {
        const err = e as Error & { statusCode?: number };
        const status = err.statusCode && err.statusCode >= 400 ? err.statusCode : 500;
        const message = status >= 500 ? "Internal Server Error" : err.message || "Bad Request";
        reply.code(status).send({
          ok: false,
          errorCode:
            status === 400
              ? "VALIDATION_FAILED"
              : status === 401
              ? "AUTH_REQUIRED"
              : "INTERNAL_ERROR",
          message,
        });
      }
    }
  );
}
```

#### æ­¥éª¤3.9ï¼šåˆ›å»ºä¸»å…¥å£æ–‡ä»¶

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/index.ts`

```typescript
import Fastify, { FastifyInstance } from "fastify";
import cors from "@fastify/cors";
import { loadConfig, type LocalAIConfig } from "./lib/config.js";
import { logger } from "./lib/logger.js";

declare module "fastify" {
  interface FastifyInstance {
    config: LocalAIConfig;
  }
}

function buildServer(config: LocalAIConfig): FastifyInstance {
  const app = Fastify({
    logger: logger,
  });

  app.decorate("config", config);

  // æ³¨å†Œ CORS
  app.register(cors, {
    origin: false, // é»˜è®¤å…³é—­è·¨åŸŸï¼Œä»…æ¥å—å†…éƒ¨è¯·æ±‚
  });

  // å¥åº·æ£€æŸ¥
  app.get("/healthz", async (_req, reply) => {
    reply.send({
      ok: true,
      data: {
        status: "ok",
        version: config.version,
        model: config.aiModel,
        embeddingModel: config.embeddingModel,
        env: config.nodeEnv,
        time: new Date().toISOString(),
      },
    });
  });

  return app;
}

async function registerRoutes(app: FastifyInstance): Promise<void> {
  try {
    const askModule = await import("./routes/ask.js");
    await askModule.default(app);
  } catch (err) {
    logger.error("è·¯ç”±æ³¨å†Œå¤±è´¥:", err);
  }
}

async function start() {
  const config = loadConfig();
  const app = buildServer(config);

  // æ³¨å†Œè·¯ç”±
  await registerRoutes(app);

  // ä¼˜é›…é€€å‡º
  const close = async () => {
    try {
      await app.close();
      process.exit(0);
    } catch (e) {
      process.exit(1);
    }
  };
  process.on("SIGINT", close);
  process.on("SIGTERM", close);

  // å¯åŠ¨æœåŠ¡
  const port = config.port;
  const host = config.host;

  try {
    await app.listen({ port, host });
    logger.info(`æœ¬åœ°AIæœåŠ¡å¯åŠ¨æˆåŠŸ: http://${host}:${port}`);
    logger.info(`Chatæ¨¡å‹: ${config.aiModel}`);
    logger.info(`Embeddingæ¨¡å‹: ${config.embeddingModel}`);
  } catch (err) {
    logger.error("æœåŠ¡å¯åŠ¨å¤±è´¥:", err);
    process.exit(1);
  }
}

process.on("unhandledRejection", (reason) => {
  logger.error("Unhandled rejection:", reason);
});

start();
```

#### æ­¥éª¤3.10ï¼šåˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶

**æ–‡ä»¶**ï¼š`apps/local-ai-service/.env.local`

```bash
# Ollama é…ç½®
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_API_KEY=ollama

# æ¨¡å‹é…ç½®
AI_MODEL=llama3.2:3b
EMBEDDING_MODEL=nomic-embed-text

# æœåŠ¡é…ç½®
PORT=8788
HOST=0.0.0.0
NODE_ENV=development
SERVICE_TOKENS=local_ai_token_dev_12345

# æ•°æ®åº“é…ç½®
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your_service_key
```

#### æ­¥éª¤3.11ï¼šå®‰è£…ä¾èµ–

```bash
cd apps/local-ai-service
npm install
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ
- âœ… æ‰€æœ‰æ–‡ä»¶åˆ›å»ºå®Œæˆ
- âœ… ä¾èµ–å®‰è£…æˆåŠŸ

---

### é˜¶æ®µ4ï¼šä¿®æ”¹ä¸»ç«™è·¯ç”±å®ç°æ— ç¼åˆ‡æ¢ï¼ˆ20åˆ†é’Ÿï¼‰

#### æ­¥éª¤4.1ï¼šä¿®æ”¹ç°æœ‰ `/api/ai/ask` è·¯ç”±æ”¯æŒæœåŠ¡åˆ‡æ¢

**æ–‡ä»¶**ï¼š`apps/web/app/api/ai/ask/route.ts`

**ä¿®æ”¹ä½ç½®**ï¼šåœ¨æ­¥éª¤3ï¼ˆè½¬å‘åˆ° AI-Serviceï¼‰éƒ¨åˆ†

**ä¿®æ”¹å†…å®¹**ï¼šæ·»åŠ æœåŠ¡é€‰æ‹©é€»è¾‘

```typescript
// åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ é…ç½®è¯»å–
const USE_LOCAL_AI = process.env.USE_LOCAL_AI === "true"; // æ–°å¢ï¼šæ§åˆ¶æ˜¯å¦ä½¿ç”¨æœ¬åœ°AI
const LOCAL_AI_SERVICE_URL = process.env.LOCAL_AI_SERVICE_URL ?? "";
const LOCAL_AI_SERVICE_TOKEN = process.env.LOCAL_AI_SERVICE_TOKEN ?? "";

// åœ¨æ­¥éª¤3å¤„ä¿®æ”¹ï¼š
// 3) é€‰æ‹©AIæœåŠ¡ï¼ˆæœ¬åœ°æˆ–åœ¨çº¿ï¼‰
const useLocalAI = USE_LOCAL_AI && LOCAL_AI_SERVICE_URL && LOCAL_AI_SERVICE_TOKEN;
const aiServiceUrl = useLocalAI ? LOCAL_AI_SERVICE_URL : AI_SERVICE_URL;
const aiServiceToken = useLocalAI ? LOCAL_AI_SERVICE_TOKEN : AI_SERVICE_TOKEN;

if (!aiServiceUrl || !aiServiceToken) {
  return internalError("AI service is not configured.");
}

// è½¬å‘åˆ°é€‰æ‹©çš„AIæœåŠ¡
// æ³¨æ„ï¼šæœ¬åœ°AIæœåŠ¡å’Œåœ¨çº¿AIæœåŠ¡ä½¿ç”¨ç›¸åŒçš„æ¥å£æ ¼å¼
aiResp = await fetch(`${aiServiceUrl.replace(/\/$/, "")}/v1/ask`, {
  method: "POST",
  headers: {
    "content-type": "application/json; charset=utf-8",
    authorization: `Bearer ${aiServiceToken}`,
  },
  body: JSON.stringify(forwardPayload),
});
```

**å…³é”®ç‚¹**ï¼š
- âœ… é€šè¿‡ç¯å¢ƒå˜é‡ `USE_LOCAL_AI=true` æ§åˆ¶åˆ‡æ¢
- âœ… å‰ç«¯ä»£ç å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
- âœ… æ¥å£æ ¼å¼å®Œå…¨å…¼å®¹
- âœ… æ”¯æŒåœ¨çº¿AIæœåŠ¡ä½œä¸ºé™çº§æ–¹æ¡ˆ

#### æ­¥éª¤4.2ï¼šæ›´æ–°ä¸»ç«™ç¯å¢ƒå˜é‡é…ç½®

**æ–‡ä»¶**ï¼š`.env.local`ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰

```bash
# AIæœåŠ¡é€‰æ‹©ï¼ˆæ–°å¢ï¼šæ§åˆ¶ä½¿ç”¨å“ªä¸ªæœåŠ¡ï¼‰
USE_LOCAL_AI=true  # è®¾ç½®ä¸º true ä½¿ç”¨æœ¬åœ°AIï¼Œfalse æˆ–æœªè®¾ç½®ä½¿ç”¨åœ¨çº¿AI

# æœ¬åœ°AIæœåŠ¡é…ç½®ï¼ˆæ–°å¢ï¼‰
LOCAL_AI_SERVICE_URL=http://localhost:8788
LOCAL_AI_SERVICE_TOKEN=local_ai_token_dev_12345

# åœ¨çº¿AIæœåŠ¡é…ç½®ï¼ˆä¿ç•™ï¼Œä½œä¸ºé™çº§æ–¹æ¡ˆï¼‰
AI_SERVICE_URL=https://ai.zalem.app
AI_SERVICE_TOKEN=svc_token_xxx
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… ä¸»ç«™è·¯ç”±æ”¯æŒæ— ç¼åˆ‡æ¢
- âœ… ç¯å¢ƒå˜é‡é…ç½®å®Œæˆ
- âœ… å‰ç«¯ä»£ç æ— éœ€ä¿®æ”¹

---

### é˜¶æ®µ4.5ï¼šç¡®ä¿æœ¬åœ°AIæœåŠ¡æ¥å£æ ¼å¼å…¼å®¹ï¼ˆ10åˆ†é’Ÿï¼‰

#### æ­¥éª¤4.5.1ï¼šæ£€æŸ¥å¹¶è°ƒæ•´æœ¬åœ°AIæœåŠ¡å“åº”æ ¼å¼

**æ–‡ä»¶**ï¼š`apps/local-ai-service/src/routes/ask.ts`

**ç¡®ä¿å“åº”æ ¼å¼ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨ä¸€è‡´**ï¼š

```typescript
// å“åº”æ ¼å¼å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼ˆä¸åœ¨çº¿AIæœåŠ¡å¯¹é½ï¼‰
type AskResult = {
  answer: string;
  sources?: Array<{ title: string; url: string; snippet?: string }>;
  model: string;
  safetyFlag: "ok" | "needs_human" | "blocked";
  // å¯é€‰ï¼šå‘åå…¼å®¹å­—æ®µ
  reference?: string | null;
  tokens?: { prompt?: number; completion?: number; total?: number };
  lang?: string;
  cached?: boolean;
  time?: string;
};

// è¿”å›æ ¼å¼
reply.send({
  ok: true,
  data: {
    answer,
    sources: sources.length > 0 ? sources : undefined,
    model: config.aiModel,
    safetyFlag: "ok",
    // å‘åå…¼å®¹å­—æ®µ
    reference: reference || null,
    lang,
    cached: false,
    time: new Date().toISOString(),
  },
});
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… æœ¬åœ°AIæœåŠ¡å“åº”æ ¼å¼ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨ä¸€è‡´
- âœ… ä¸»ç«™å¯ä»¥æ— ç¼åˆ‡æ¢æœåŠ¡

---

### é˜¶æ®µ4.6ï¼šéªŒè¯æ— ç¼åˆ‡æ¢ï¼ˆ5åˆ†é’Ÿï¼‰

**éªŒè¯æ­¥éª¤**ï¼š
1. ç¡®ä¿ç¯å¢ƒå˜é‡ `USE_LOCAL_AI=true` å·²è®¾ç½®
2. å¯åŠ¨æœ¬åœ°AIæœåŠ¡ï¼ˆ`localhost:8788`ï¼‰
3. å¯åŠ¨ä¸»ç«™ï¼ˆ`localhost:3000`ï¼‰
4. å‰ç«¯è°ƒç”¨ `/api/ai/ask`ï¼Œåº”è¯¥è‡ªåŠ¨ä½¿ç”¨æœ¬åœ°AIæœåŠ¡
5. ä¿®æ”¹ `USE_LOCAL_AI=false`ï¼Œåº”è¯¥è‡ªåŠ¨åˆ‡æ¢å›åœ¨çº¿AIæœåŠ¡

**é¢„æœŸç»“æœ**ï¼š
- âœ… é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶æ— ç¼åˆ‡æ¢
- âœ… å‰ç«¯ä»£ç å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
- âœ… æ¥å£æ ¼å¼å®Œå…¨å…¼å®¹

---

### é˜¶æ®µ5ï¼šæµ‹è¯•éªŒè¯ï¼ˆ20åˆ†é’Ÿï¼‰

#### æ­¥éª¤5.1ï¼šå¯åŠ¨æœåŠ¡

```bash
# ç»ˆç«¯1: å¯åŠ¨ Ollamaï¼ˆå¦‚æœè¿˜æ²¡å¯åŠ¨ï¼‰
ollama serve

# ç»ˆç«¯2: å¯åŠ¨æœ¬åœ°AIæœåŠ¡
cd apps/local-ai-service
npm run dev

# ç»ˆç«¯3: å¯åŠ¨ä¸»ç«™
npm run dev
```

#### æ­¥éª¤5.2ï¼šæµ‹è¯•æœ¬åœ°AIæœåŠ¡

```bash
# 1. å¥åº·æ£€æŸ¥
curl http://localhost:8788/healthz

# é¢„æœŸç»“æœ:
# {"ok":true,"data":{"status":"ok","version":"1.0.0","model":"llama3.2:3b","embeddingModel":"nomic-embed-text",...}}

# 2. æµ‹è¯•é—®ç­”ï¼ˆéœ€è¦tokenï¼‰
curl -X POST http://localhost:8788/v1/ask \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer local_ai_token_dev_12345" \
  -d '{
    "question": "æ—¥æœ¬é©¾è€ƒä¸­ï¼Œè¶…é€Ÿè¡Œé©¶çš„å¤„ç½šæ˜¯ä»€ä¹ˆï¼Ÿ",
    "lang": "zh"
  }'
```

#### æ­¥éª¤5.3ï¼šæµ‹è¯•ä¸»ç«™è·¯ç”±ï¼ˆæ— ç¼åˆ‡æ¢ï¼‰

```bash
# æµ‹è¯•ä¸»ç«™è·¯ç”±ï¼ˆä½¿ç”¨ç°æœ‰ /api/ai/askï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æœ¬åœ°AIï¼‰
curl -X POST http://localhost:3000/api/ai/ask \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_jwt_token" \
  -d '{
    "question": "æ—¥æœ¬é©¾è€ƒä¸­ï¼Œè¶…é€Ÿè¡Œé©¶çš„å¤„ç½šæ˜¯ä»€ä¹ˆï¼Ÿ",
    "locale": "zh-CN"
  }'

# éªŒè¯ï¼šå“åº”æ ¼å¼åº”è¯¥ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨ä¸€è‡´
# é¢„æœŸå“åº”æ ¼å¼ï¼š
# {
#   "ok": true,
#   "data": {
#     "answer": "...",
#     "sources": [...],
#     "model": "llama3.2:3b",
#     "safetyFlag": "ok",
#     ...
#   }
# }
```

#### æ­¥éª¤5.4ï¼šæµ‹è¯• Ollama æ¨¡å‹

```bash
# æµ‹è¯• Chat æ¨¡å‹
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
  }'

# æµ‹è¯• Embedding æ¨¡å‹
curl http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "æ—¥æœ¬äº¤é€šæ³•è§„"
  }'
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… æ‰€æœ‰æœåŠ¡å¯åŠ¨æˆåŠŸ
- âœ… å¥åº·æ£€æŸ¥è¿”å›æ­£å¸¸
- âœ… é—®ç­”æ¥å£è¿”å›ç­”æ¡ˆ
- âœ… Ollama æ¨¡å‹æ­£å¸¸å·¥ä½œ

---

## âœ… é¢„æœŸç»“æœ

### 1. æ¶æ„å®Œæ•´æ€§

- âœ… ç‹¬ç«‹çš„ `apps/local-ai-service` æœåŠ¡
- âœ… ç‹¬ç«‹çš„ç«¯å£ `8788`ï¼ˆä¸ä¸ `apps/ai-service` å†²çªï¼‰
- âœ… ä¿®æ”¹ç°æœ‰è·¯ç”± `/api/ai/ask` æ”¯æŒæ— ç¼åˆ‡æ¢ï¼ˆä¸»ç«™ï¼‰
- âœ… ç‹¬ç«‹çš„é…ç½®å’Œç¯å¢ƒå˜é‡

### 2. åŠŸèƒ½å®Œæ•´æ€§

- âœ… æœ¬åœ°AIæœåŠ¡å¯ä»¥ç‹¬ç«‹è¿è¡Œ
- âœ… æ”¯æŒ RAG æ£€ç´¢ï¼ˆ768ç»´å‘é‡ï¼‰
- âœ… æ”¯æŒå¤šè¯­è¨€é—®ç­”ï¼ˆzh/ja/enï¼‰
- âœ… å¥åº·æ£€æŸ¥æ¥å£æ­£å¸¸
- âœ… é”™è¯¯å¤„ç†å®Œå–„
- âœ… **æ— ç¼åˆ‡æ¢**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ä½¿ç”¨æœ¬åœ°æˆ–åœ¨çº¿AIæœåŠ¡
- âœ… **æ¥å£å…¼å®¹**ï¼šå“åº”æ ¼å¼ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨ä¸€è‡´
- âœ… **å‰ç«¯æ— æ„Ÿ**ï¼šå‰ç«¯ä»£ç å®Œå…¨ä¸éœ€è¦ä¿®æ”¹

### 3. æ€§èƒ½æŒ‡æ ‡

- âœ… æœåŠ¡å¯åŠ¨æ—¶é—´ï¼š< 5ç§’
- âœ… é—®ç­”å“åº”æ—¶é—´ï¼š< 5ç§’ï¼ˆæœ¬åœ°ç½‘ç»œï¼‰
- âœ… Embedding ç”Ÿæˆæ—¶é—´ï¼š< 1ç§’
- âœ… RAG æ£€ç´¢æ—¶é—´ï¼š< 500ms

### 4. ç‹¬ç«‹æ€§éªŒè¯

- âœ… ä¸ä¾èµ– `apps/ai-service` çš„ä»£ç 
- âœ… ä¸å…±äº«é…ç½®æ–‡ä»¶
- âœ… å¯ä»¥åŒæ—¶è¿è¡Œä¸¤ä¸ªæœåŠ¡ï¼ˆ8787 å’Œ 8788ï¼‰
- âœ… ä¸»ç«™å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ `USE_LOCAL_AI` æ— ç¼åˆ‡æ¢æœåŠ¡
- âœ… å‰ç«¯ä»£ç å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
- âœ… æ¥å£æ ¼å¼å®Œå…¨å…¼å®¹ï¼Œæ”¯æŒæ— ç¼åˆ‡æ¢

---

## ğŸ“‹ éªŒè¯æ¸…å•

æ‰§è¡Œå®Œæˆåï¼Œè¯·éªŒè¯ä»¥ä¸‹é¡¹ç›®ï¼š

- [ ] Ollama æœåŠ¡è¿è¡Œåœ¨ `localhost:11434`
- [ ] `llama3.2:3b` å’Œ `nomic-embed-text` æ¨¡å‹å·²ä¸‹è½½
- [ ] æ•°æ®åº“è¿ç§»å®Œæˆï¼ˆ`ai_vectors.embedding` ä¸º `vector(768)`ï¼‰
- [ ] `apps/local-ai-service` ç›®å½•ç»“æ„å®Œæ•´
- [ ] æ‰€æœ‰ TypeScript æ–‡ä»¶ç¼–è¯‘é€šè¿‡
- [ ] æœ¬åœ°AIæœåŠ¡å¯åŠ¨åœ¨ `localhost:8788`
- [ ] å¥åº·æ£€æŸ¥ `/healthz` è¿”å›æ­£å¸¸
- [ ] é—®ç­”æ¥å£ `/v1/ask` è¿”å›ç­”æ¡ˆ
- [ ] ä¸»ç«™è·¯ç”± `/api/ai/ask` æ”¯æŒæ— ç¼åˆ‡æ¢åˆ°æœ¬åœ°AIæœåŠ¡
- [ ] é€šè¿‡ç¯å¢ƒå˜é‡ `USE_LOCAL_AI=true` å¯ç”¨æœ¬åœ°AI
- [ ] å‰ç«¯è°ƒç”¨ `/api/ai/ask` æ­£å¸¸å·¥ä½œï¼ˆæ— éœ€ä¿®æ”¹å‰ç«¯ä»£ç ï¼‰
- [ ] ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®
- [ ] æ—¥å¿—è¾“å‡ºæ­£å¸¸

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å®Œå…¨ç‹¬ç«‹**ï¼šæœ¬åœ°AIæœåŠ¡ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨è§£è€¦
2. **æ–¹æ¡ˆA**ï¼šä½¿ç”¨ `llama3.2:3b` + `nomic-embed-text`ï¼ˆ768ç»´ï¼‰
3. **æœ¬åœ°åŒ–**ï¼šæ‰€æœ‰AIå¤„ç†éƒ½åœ¨æœ¬åœ°å®Œæˆï¼Œæ— éœ€äº‘ç«¯API
4. **æ— ç¼åˆ‡æ¢**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡ `USE_LOCAL_AI=true` æ§åˆ¶åˆ‡æ¢ï¼Œå‰ç«¯æ— éœ€ä¿®æ”¹
5. **æ¥å£å…¼å®¹**ï¼šæœ¬åœ°AIæœåŠ¡å“åº”æ ¼å¼ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨ä¸€è‡´
6. **å¯æ‰©å±•**ï¼šæœªæ¥å¯ä»¥è½»æ¾åˆ‡æ¢æ¨¡å‹æˆ–å‡çº§é…ç½®

### æ³¨æ„äº‹é¡¹

- âš ï¸ ä¸è¦ä¿®æ”¹ `apps/ai-service` çš„ä»»ä½•ä»£ç 
- âš ï¸ ç¡®ä¿ç«¯å£ä¸å†²çªï¼ˆ8787 vs 8788ï¼‰
- âš ï¸ ç¯å¢ƒå˜é‡è¦åŒºåˆ†ï¼ˆ`LOCAL_AI_*` vs `AI_SERVICE_*`ï¼‰
- âš ï¸ é€šè¿‡ `USE_LOCAL_AI=true` å¯ç”¨æœ¬åœ°AIæœåŠ¡
- âš ï¸ ç¡®ä¿æœ¬åœ°AIæœåŠ¡å“åº”æ ¼å¼ä¸åœ¨çº¿AIæœåŠ¡å®Œå…¨ä¸€è‡´
- âš ï¸ æ•°æ®åº“è¿ç§»å‰ç¡®ä¿æ•°æ®å·²å¤‡ä»½æˆ–ä¸ºç©º

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**: 2025-01-15  
**æœ€åæ›´æ–°**: 2025-01-15

